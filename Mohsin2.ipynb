{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load ERCOT electricity load data\n",
    "ercot_data = pd.read_excel(r'D:\\1\\Load\\DataSet\\LoadTexasERCOT.xlsx')\n",
    "\n",
    "# Convert 'Hour_End' to datetime and set it as index\n",
    "ercot_data['Hour_End'] = pd.to_datetime(ercot_data['Hour_End'])\n",
    "ercot_data.set_index('Hour_End', inplace=True)\n",
    "\n",
    "# Load weather data\n",
    "weather_data1 = pd.read_excel(r'D:\\1\\Load\\DataSet\\WeatherStation1.xlsx')\n",
    "weather_data2 = pd.read_excel(r'D:\\1\\Load\\DataSet\\WeatherStation2.xlsx')\n",
    "weather_data3 = pd.read_excel(r'D:\\1\\Load\\DataSet\\WeatherStation3.xlsx')\n",
    "\n",
    "# Convert Hour, Minute, and Date columns to datetime for weather data\n",
    "for weather_data in [weather_data1, weather_data2, weather_data3]:\n",
    "    weather_data['Datetime'] = pd.to_datetime(\n",
    "        weather_data[['Year', 'Month', 'Day', 'Hour', 'Minute']]\n",
    "    )\n",
    "    weather_data.set_index('Datetime', inplace=True)\n",
    "\n",
    "# Merge weather data with ERCOT data on timestamp\n",
    "merged_data = ercot_data.copy()\n",
    "for weather_data in [weather_data1, weather_data2, weather_data3]:\n",
    "    merged_data = merged_data.merge(weather_data, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Fill missing values\n",
    "merged_data.fillna(method='ffill', inplace=True)\n",
    "merged_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Normalize/scale features (excluding target 'ERCOT')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "features = merged_data.drop(columns=['ERCOT'])\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=features.columns, index=features.index)\n",
    "\n",
    "# Add the target variable back\n",
    "scaled_data['ERCOT'] = merged_data['ERCOT']\n",
    "\n",
    "print(\"Preprocessed dataset is ready!\")\n",
    "print(scaled_data.head())\n",
    "\n",
    "# Load ERCOT Electricity Load Data\n",
    "data = pd.read_excel(r'D:\\1\\Load\\DataSet\\LoadTexasERCOT.xlsx')\n",
    "\n",
    "# Convert timestamp column to datetime\n",
    "data['Hour_End'] = pd.to_datetime(data['Hour_End'])\n",
    "data.set_index('Hour_End', inplace=True)\n",
    "\n",
    "# Extract Features\n",
    "data['Hour'] = data.index.hour\n",
    "data['Day'] = data.index.day\n",
    "data['Month'] = data.index.month\n",
    "data['Weekday'] = data.index.weekday\n",
    "\n",
    "# Target Variable: ERCOT Load\n",
    "ercot_values = data['ERCOT']\n",
    "\n",
    "# Train-Test Split (2012-2014 for training, 2015 for testing)\n",
    "train_data = ercot_values['2012':'2014']\n",
    "test_data = ercot_values['2015']\n",
    "\n",
    "# Normalize the data (Min-Max Scaling)\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "test_scaled = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x.append(data[i:i + seq_length])  # Past sequence length data\n",
    "        y.append(data[i + seq_length])    # Target value\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "seq_length = 24  # Use past 24 hours to predict the next value\n",
    "x_train, y_train = create_sequences(train_scaled, seq_length)\n",
    "x_test, y_test = create_sequences(test_scaled, seq_length)\n",
    "\n",
    "# Reshape inputs to fit GRU model\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "model = Sequential([\n",
    "    GRU(100, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),  # First GRU Layer\n",
    "    Dropout(0.2),  \n",
    "    GRU(50, activation='relu', return_sequences=False),  # Second GRU Layer\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),  # Fully connected layer\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "predictions_scaled = model.predict(x_test)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Reshape before inverse transform\n",
    "predictions = scaler.inverse_transform(predictions_scaled.reshape(-1, 1))\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Compute error metrics\n",
    "mae = mean_absolute_error(y_test_actual, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_actual, predictions))\n",
    "mape = np.mean(np.abs((y_test_actual - predictions) / y_test_actual)) * 100\n",
    "r2 = r2_score(y_test_actual, predictions)\n",
    "\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAPE: {mape}%\")\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_actual[:500], label='Actual', linewidth=2)\n",
    "plt.plot(predictions[:500], label='Predicted', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Load (ERCOT)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Load (MW)')\n",
    "plt.show()\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_test_actual, predictions)\n",
    "mae = mean_absolute_error(y_test_actual, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_actual, predictions))\n",
    "mape = np.mean(np.abs((y_test_actual - predictions) / y_test_actual)) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"R²: {r2}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAPE: {mape}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abc\\AppData\\Local\\Temp\\ipykernel_16128\\3542382586.py:30: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.fillna(method='ffill', inplace=True)\n",
      "C:\\Users\\abc\\AppData\\Local\\Temp\\ipykernel_16128\\3542382586.py:31: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data.fillna(method='bfill', inplace=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'2015'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32md:\\1\\alpha\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '2015'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Train-Test Split\u001b[39;00m\n\u001b[0;32m     47\u001b[0m train \u001b[38;5;241m=\u001b[39m scaled_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2012\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2014\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 48\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2015\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Sequence Creation\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_sequences\u001b[39m(data, seq_length):\n",
      "File \u001b[1;32md:\\1\\alpha\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32md:\\1\\alpha\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '2015'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load ERCOT Electricity Load Data\n",
    "ercot_data = pd.read_excel('D:/1/Load/DataSet/LoadTexasERCOT.xlsx')\n",
    "ercot_data['Hour_End'] = pd.to_datetime(ercot_data['Hour_End'])\n",
    "ercot_data.set_index('Hour_End', inplace=True)\n",
    "\n",
    "# Load weather data\n",
    "weather_dfs = []\n",
    "for i in range(1, 4):\n",
    "    df = pd.read_excel(f'D:/1/Load/DataSet/WeatherStation{i}.xlsx')\n",
    "    df['Datetime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute']])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    weather_dfs.append(df)\n",
    "\n",
    "# Merge all datasets\n",
    "merged_data = ercot_data.copy()\n",
    "for df in weather_dfs:\n",
    "    merged_data = merged_data.merge(df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Handle missing values\n",
    "merged_data.fillna(method='ffill', inplace=True)\n",
    "merged_data.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "merged_data['Hour'] = merged_data.index.hour\n",
    "merged_data['Day'] = merged_data.index.day\n",
    "merged_data['Month'] = merged_data.index.month\n",
    "merged_data['Weekday'] = merged_data.index.weekday\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "features = merged_data.drop(columns=['ERCOT'])\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=features.columns, index=features.index)\n",
    "scaled_data['ERCOT'] = merged_data['ERCOT']\n",
    "\n",
    "# Train-Test Split\n",
    "train = scaled_data['2012':'2014']\n",
    "test = scaled_data['2015']\n",
    "\n",
    "# Sequence Creation\n",
    "def create_sequences(data, seq_length):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "seq_length = 24\n",
    "x_train, y_train = create_sequences(train['ERCOT'].values, seq_length)\n",
    "x_test, y_test = create_sequences(test['ERCOT'].values, seq_length)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)\n",
    "\n",
    "# Build Improved GRU Model\n",
    "model = Sequential([\n",
    "    GRU(128, return_sequences=True, activation='relu', input_shape=(seq_length, 1)),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    GRU(64, return_sequences=False, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Predictions\n",
    "predictions_scaled = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions_scaled.reshape(-1, 1))\n",
    "y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Compute Metrics\n",
    "mae = mean_absolute_error(y_test_actual, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_actual, predictions))\n",
    "mape = np.mean(np.abs((y_test_actual - predictions) / y_test_actual)) * 100\n",
    "r2 = r2_score(y_test_actual, predictions)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_actual[:500], label='Actual', linewidth=2)\n",
    "plt.plot(predictions[:500], label='Predicted', linestyle='dashed')\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Load (ERCOT)')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Load (MW)')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of Errors\n",
    "errors = y_test_actual - predictions\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(errors, bins=50, kde=True)\n",
    "plt.title('Error Distribution')\n",
    "plt.xlabel('Error')\n",
    "plt.show()\n",
    "\n",
    "# Print Results\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAPE: {mape}%\")\n",
    "print(f\"R² Score: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
